{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6e9xTF154t4afZuNaceEg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/feridjabri/diffusion/blob/main/Copie_de_diff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RNLTzzoHUzZ",
        "outputId": "90016310-0414-444c-99ad-f0175650a775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4crZRU1Z4ZG",
        "outputId": "a1b97f6b-bc8a-4c25-9e28-c2819e437e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gdrive\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gdrive/My Drive/DiffusionInst-main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTLpSEaXZ7eG",
        "outputId": "81978145-56bd-419c-916f-05cfb5f493c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/DiffusionInst-main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/feridjabri/diffusion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kputicVZ8ue",
        "outputId": "dbcc20e8-1aac-4254-c536-c4aa725ead38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'diffusion'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 41 (delta 6), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (41/41), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install diffusion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMFUTjGqZ8Ye",
        "outputId": "95093a74-fd9c-46ca-b3f5-d60bc1836064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting diffusion\n",
            "  Downloading diffusion-6.8.8-1-py3-none-any.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 432 kB/s \n",
            "\u001b[?25hCollecting structlog==20.*,>=20.1.0\n",
            "  Downloading structlog-20.2.0-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting diffusion-core==0.0.16\n",
            "  Downloading diffusion_core-0.0.16-1-cp38-cp38-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (183 kB)\n",
            "\u001b[K     |████████████████████████████████| 183 kB 20.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions==4.*,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from diffusion) (4.4.0)\n",
            "Requirement already satisfied: pydantic==1.*,>=1.8.2 in /usr/local/lib/python3.8/dist-packages (from diffusion) (1.10.2)\n",
            "Collecting cbor2==5.*,>=5.1.2\n",
            "  Downloading cbor2-5.4.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n",
            "\u001b[K     |████████████████████████████████| 236 kB 38.7 MB/s \n",
            "\u001b[?25hCollecting stringcase==1.*,>=1.2.0\n",
            "  Downloading stringcase-1.2.0.tar.gz (3.0 kB)\n",
            "Requirement already satisfied: aiohttp==3.*,>=3.6.2 in /usr/local/lib/python3.8/dist-packages (from diffusion) (3.8.3)\n",
            "Collecting attrs==20.*,>=20.1.0\n",
            "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.*,>=3.6.2->diffusion) (6.0.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.*,>=3.6.2->diffusion) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.*,>=3.6.2->diffusion) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.*,>=3.6.2->diffusion) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.*,>=3.6.2->diffusion) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.*,>=3.6.2->diffusion) (1.3.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp==3.*,>=3.6.2->diffusion) (2.10)\n",
            "Building wheels for collected packages: stringcase\n",
            "  Building wheel for stringcase (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stringcase: filename=stringcase-1.2.0-py3-none-any.whl size=3587 sha256=ba5975d4ba7cf60a3fef0c7fbb7d9f106cda682de4528ebe86932635c841971a\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/0e/31/bf265c64f2a4d24516e9923f1f6293c3bcbcde75e0d80ab47a\n",
            "Successfully built stringcase\n",
            "Installing collected packages: attrs, structlog, stringcase, diffusion-core, cbor2, diffusion\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 22.1.0\n",
            "    Uninstalling attrs-22.1.0:\n",
            "      Successfully uninstalled attrs-22.1.0\n",
            "Successfully installed attrs-20.3.0 cbor2-5.4.6 diffusion-6.8.8 diffusion-core-0.0.16 stringcase-1.2.0 structlog-20.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities.\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrcQijLea8um",
        "outputId": "62b8c36b-e1f7-472c-8026-77f2f57de11c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyyaml==5.1\n",
            "  Downloading PyYAML-5.1.tar.gz (274 kB)\n",
            "\u001b[K     |████████████████████████████████| 274 kB 7.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1-cp38-cp38-linux_x86_64.whl size=44089 sha256=aed96ad1409164df0734dace52f3fda952d93eb4e68dc289d9fa7766d981742f\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/dd/2b/10ff8b0ac81b93946bb5fb9e6749bae2dac246506c8774e6cf\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dask 2022.2.1 requires pyyaml>=5.3.1, but you have pyyaml 5.1 which is incompatible.\u001b[0m\n",
            "Successfully installed pyyaml-5.1\n",
            "Cloning into 'detectron2'...\n",
            "remote: Enumerating objects: 14651, done.\u001b[K\n",
            "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 14651 (delta 25), reused 45 (delta 14), pack-reused 14584\u001b[K\n",
            "Receiving objects: 100% (14651/14651), 6.01 MiB | 4.35 MiB/s, done.\n",
            "Resolving deltas: 100% (10581/10581), done.\n",
            "Checking out files: 100% (819/819), done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.8/dist-packages (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.8/dist-packages (2.0.6)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.8/dist-packages (2.1.1)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.8/dist-packages (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (0.8.10)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (1.5.0)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (2.9.1)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.8/dist-packages (0.1.5.post20221213)\n",
            "Collecting iopath<0.1.10,>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting omegaconf>=2.1\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting hydra-core>=1.1\n",
            "  Downloading hydra_core-1.3.0-py3-none-any.whl (153 kB)\n",
            "\u001b[K     |████████████████████████████████| 153 kB 39.1 MB/s \n",
            "\u001b[?25hCollecting black\n",
            "  Downloading black-22.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 39.8 MB/s \n",
            "\u001b[?25hCollecting timm\n",
            "  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n",
            "\u001b[K     |████████████████████████████████| 549 kB 52.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pycocotools>=2.0.2) (1.21.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from yacs>=0.1.8) (5.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.8/dist-packages (from iopath<0.1.10,>=0.1.7) (2.6.0)\n",
            "Collecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 52.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from hydra-core>=1.1) (5.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (2.23.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.3.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (2.15.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (3.19.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.51.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.38.4)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.4.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.8/dist-packages (from black) (2.5.4)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from black) (2.0.1)\n",
            "Collecting pathspec>=0.9.0\n",
            "  Downloading pathspec-0.10.3-py3-none-any.whl (29 kB)\n",
            "Collecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from black) (4.4.0)\n",
            "Collecting click>=8.0.0\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 5.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 78.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.8/dist-packages (from timm) (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from timm) (0.14.0+cu116)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (3.8.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=a98b09d3c2539c7e1df6558306c851f6435bd5bf5b4f8ca911693a7ef51dda6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/a3/c2/6df046c09459b73cc9bb6c4401b0be6c47048baf9a1617c485\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, pathspec, omegaconf, mypy-extensions, iopath, huggingface-hub, click, timm, hydra-core, black\n",
            "  Attempting uninstall: iopath\n",
            "    Found existing installation: iopath 0.1.10\n",
            "    Uninstalling iopath-0.1.10:\n",
            "      Successfully uninstalled iopath-0.1.10\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.1.3 which is incompatible.\u001b[0m\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-22.12.0 click-8.1.3 huggingface-hub-0.11.1 hydra-core-1.3.0 iopath-0.1.9 mypy-extensions-0.4.3 omegaconf-2.3.0 pathspec-0.10.3 timm-0.6.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install diffusion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POI2JpxgcXTl",
        "outputId": "77f39b0d-cb59-4000-d3cc-46c7a8d8230d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: diffusion in /usr/local/lib/python3.8/dist-packages (6.8.8)\n",
            "Requirement already satisfied: aiohttp==3.*,>=3.6.2 in /usr/local/lib/python3.8/dist-packages (from diffusion) (3.8.3)\n",
            "Requirement already satisfied: cbor2==5.*,>=5.1.2 in /usr/local/lib/python3.8/dist-packages (from diffusion) (5.4.6)\n",
            "Requirement already satisfied: stringcase==1.*,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from diffusion) (1.2.0)\n",
            "Requirement already satisfied: structlog==20.*,>=20.1.0 in /usr/local/lib/python3.8/dist-packages (from diffusion) (20.2.0)\n",
            "Requirement already satisfied: diffusion-core==0.0.16 in /usr/local/lib/python3.8/dist-packages (from diffusion) (0.0.16)\n",
            "Requirement already satisfied: attrs==20.*,>=20.1.0 in /usr/local/lib/python3.8/dist-packages (from diffusion) (20.3.0)\n",
            "Requirement already satisfied: pydantic==1.*,>=1.8.2 in /usr/local/lib/python3.8/dist-packages (from diffusion) (1.10.2)\n",
            "Requirement already satisfied: typing-extensions==4.*,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from diffusion) (4.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.*,>=3.6.2->diffusion) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.*,>=3.6.2->diffusion) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.*,>=3.6.2->diffusion) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.*,>=3.6.2->diffusion) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.*,>=3.6.2->diffusion) (6.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp==3.*,>=3.6.2->diffusion) (1.8.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp==3.*,>=3.6.2->diffusion) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python <executable_file.py>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzj8cgO9NzTt",
        "outputId": "b3c557a7-9bba-4225-a5a8-ce434edf03ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `newline'\n",
            "/bin/bash: -c: line 0: ` python <executable_file.py>'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install util\n",
        "from utilities.file_handling import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "6nq_p9IbN7R1",
        "outputId": "66bdd5ae-a2fc-4ee1-bc09-8afacb398b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement util (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for util\u001b[0m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-04e54e3d312c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' pip install util'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_handling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fvcore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLp1O_3wRNWT",
        "outputId": "8c15422d-cae1-4b89-9c25-44f703633623"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221213.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fvcore) (1.21.6)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from fvcore) (6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from fvcore) (4.64.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.8/dist-packages (from fvcore) (2.1.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from fvcore) (7.1.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from fvcore) (0.8.10)\n",
            "Collecting iopath>=0.1.7\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 793 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing_extensions in /usr/local/lib/python3.8/dist-packages (from iopath>=0.1.7->fvcore) (4.4.0)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221213-py3-none-any.whl size=61498 sha256=7a2ba390c95e86612f124873a3c6d603f77c0506e6e2c87f6fcdd3e9b144e220\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/6d/5c/4fd3efe9b62aeae1e7e68204b54487df288e58e28f3d13fa1e\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31547 sha256=0b3982f9d1a3cc07e71ed68d3b92f11d088a9791eaba5b2fe6c61613ff97a374\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/3e/24/0f349c0b2eeb6965903035f3b00dbb5c9bea437b4a2f18d82c\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: portalocker, yacs, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221213 iopath-0.1.10 portalocker-2.6.0 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyyaml==5.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mu1qtrJaReha",
        "outputId": "e62eb563-defb-42cf-b972-acefbbf5be15"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyyaml==5.1\n",
            "  Downloading PyYAML-5.1.tar.gz (274 kB)\n",
            "\u001b[K     |████████████████████████████████| 274 kB 5.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1-cp38-cp38-linux_x86_64.whl size=44089 sha256=fc10329900f976a0bcb0ac4bb37fd07a9fcaa787619bcc68f1a8a3a7cbc38dad\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/dd/2b/10ff8b0ac81b93946bb5fb9e6749bae2dac246506c8774e6cf\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dask 2022.2.1 requires pyyaml>=5.3.1, but you have pyyaml 5.1 which is incompatible.\u001b[0m\n",
            "Successfully installed pyyaml-5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyyaml 5.3.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkeDe8XvRne7",
        "outputId": "6869af6f-af64-42c3-f8bf-b130bc10e796"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (5.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement 5.3.1 (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for 5.3.1\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "ea6HpC8LD3m2",
        "outputId": "ab7886d2-02ed-4d1d-809c-977150259d7c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1a7b3a03faa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfvcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_bn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_bn_modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetectionCheckpointer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'detectron2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\"\"\"\n",
        "Modified by Zhangxuan Gu, Haoxing Chen\n",
        "Date: Nov 30, 2022\n",
        "Version: V0\n",
        "Contact: {guzhangxuan.gzx, chenhaoxing.chx}@antgroup.com\n",
        "Copyright (c) Ant Group, Inc. and its affiliates. All Rights Reserved\n",
        "\n",
        "DiffusionInst Training Script.\n",
        "\n",
        "This script is a simplified version of the training script in detectron2/tools.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import itertools\n",
        "import weakref\n",
        "from typing import Any, Dict, List, Set\n",
        "import logging\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "from fvcore.nn.precise_bn import get_bn_modules\n",
        "\n",
        "import detectron2.utils.comm as comm\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import build_detection_train_loader\n",
        "from detectron2.engine import DefaultTrainer, default_argument_parser, default_setup, launch, create_ddp_model, \\\n",
        "    AMPTrainer, SimpleTrainer, hooks\n",
        "from detectron2.evaluation import COCOEvaluator, LVISEvaluator, verify_results\n",
        "from detectron2.solver.build import maybe_add_gradient_clipping\n",
        "from detectron2.modeling import build_model\n",
        "\n",
        "from diffusioninst import * #DiffusionInstDatasetMapper, add_diffusioninst_config, DiffusionInstWithTTA\n",
        "from diffusioninst.util.model_ema import add_model_ema_configs, may_build_model_ema, may_get_ema_checkpointer, EMAHook, \\\n",
        "    apply_model_ema_and_restore, EMADetectionCheckpointer\n",
        "\n",
        "\n",
        "class Trainer(DefaultTrainer):\n",
        "    \"\"\" Extension of the Trainer class adapted to DiffusionInst. \"\"\"\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            cfg (CfgNode):\n",
        "        \"\"\"\n",
        "        super(DefaultTrainer, self).__init__()  # call grandfather's `__init__` while avoid father's `__init()`\n",
        "        logger = logging.getLogger(\"detectron2\")\n",
        "        if not logger.isEnabledFor(logging.INFO):  # setup_logger is not called for d2\n",
        "            setup_logger()\n",
        "        cfg = DefaultTrainer.auto_scale_workers(cfg, comm.get_world_size())\n",
        "\n",
        "        # Assume these objects must be constructed in this order.\n",
        "        model = self.build_model(cfg)\n",
        "        optimizer = self.build_optimizer(cfg, model)\n",
        "        data_loader = self.build_train_loader(cfg)\n",
        "\n",
        "        model = create_ddp_model(model, broadcast_buffers=False)\n",
        "        self._trainer = (AMPTrainer if cfg.SOLVER.AMP.ENABLED else SimpleTrainer)(\n",
        "            model, data_loader, optimizer\n",
        "        )\n",
        "\n",
        "        self.scheduler = self.build_lr_scheduler(cfg, optimizer)\n",
        "\n",
        "        ########## EMA ############\n",
        "        kwargs = {\n",
        "            'trainer': weakref.proxy(self),\n",
        "        }\n",
        "        kwargs.update(may_get_ema_checkpointer(cfg, model))\n",
        "        self.checkpointer = DetectionCheckpointer(\n",
        "            # Assume you want to save checkpoints together with logs/statistics\n",
        "            model,\n",
        "            cfg.OUTPUT_DIR,\n",
        "            **kwargs,\n",
        "            # trainer=weakref.proxy(self),\n",
        "        )\n",
        "        self.start_iter = 0\n",
        "        self.max_iter = cfg.SOLVER.MAX_ITER\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.register_hooks(self.build_hooks())\n",
        "\n",
        "    @classmethod\n",
        "    def build_model(cls, cfg):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            torch.nn.Module:\n",
        "\n",
        "        It now calls :func:`detectron2.modeling.build_model`.\n",
        "        Overwrite it if you'd like a different model.\n",
        "        \"\"\"\n",
        "        model = build_model(cfg)\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.info(\"Model:\\n{}\".format(model))\n",
        "        # setup EMA\n",
        "        may_build_model_ema(cfg, model)\n",
        "        return model\n",
        "\n",
        "    @classmethod\n",
        "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "        \"\"\"\n",
        "        Create evaluator(s) for a given dataset.\n",
        "        This uses the special metadata \"evaluator_type\" associated with each builtin dataset.\n",
        "        For your own dataset, you can simply create an evaluator manually in your\n",
        "        script and do not have to worry about the hacky if-else logic here.\n",
        "        \"\"\"\n",
        "        if output_folder is None:\n",
        "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
        "        if 'lvis' in dataset_name:\n",
        "            return LVISEvaluator(dataset_name, cfg, True, output_folder)\n",
        "        else:\n",
        "            return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
        "\n",
        "    @classmethod\n",
        "    def build_train_loader(cls, cfg):\n",
        "        mapper = DiffusionInstDatasetMapper(cfg, is_train=True)\n",
        "        return build_detection_train_loader(cfg, mapper=mapper)\n",
        "\n",
        "    @classmethod\n",
        "    def build_optimizer(cls, cfg, model):\n",
        "        params: List[Dict[str, Any]] = []\n",
        "        memo: Set[torch.nn.parameter.Parameter] = set()\n",
        "        for key, value in model.named_parameters(recurse=True):\n",
        "            if not value.requires_grad:\n",
        "                continue\n",
        "            # Avoid duplicating parameters\n",
        "            if value in memo:\n",
        "                continue\n",
        "            memo.add(value)\n",
        "            lr = cfg.SOLVER.BASE_LR\n",
        "            weight_decay = cfg.SOLVER.WEIGHT_DECAY\n",
        "            if \"backbone\" in key:\n",
        "                lr = lr * cfg.SOLVER.BACKBONE_MULTIPLIER\n",
        "            params += [{\"params\": [value], \"lr\": lr, \"weight_decay\": weight_decay}]\n",
        "\n",
        "        def maybe_add_full_model_gradient_clipping(optim):  # optim: the optimizer class\n",
        "            # detectron2 doesn't have full model gradient clipping now\n",
        "            clip_norm_val = cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE\n",
        "            enable = (\n",
        "                    cfg.SOLVER.CLIP_GRADIENTS.ENABLED\n",
        "                    and cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == \"full_model\"\n",
        "                    and clip_norm_val > 0.0\n",
        "            )\n",
        "\n",
        "            class FullModelGradientClippingOptimizer(optim):\n",
        "                def step(self, closure=None):\n",
        "                    all_params = itertools.chain(*[x[\"params\"] for x in self.param_groups])\n",
        "                    torch.nn.utils.clip_grad_norm_(all_params, clip_norm_val)\n",
        "                    super().step(closure=closure)\n",
        "\n",
        "            return FullModelGradientClippingOptimizer if enable else optim\n",
        "\n",
        "        optimizer_type = cfg.SOLVER.OPTIMIZER\n",
        "        if optimizer_type == \"SGD\":\n",
        "            optimizer = maybe_add_full_model_gradient_clipping(torch.optim.SGD)(\n",
        "                params, cfg.SOLVER.BASE_LR, momentum=cfg.SOLVER.MOMENTUM\n",
        "            )\n",
        "        elif optimizer_type == \"ADAMW\":\n",
        "            optimizer = maybe_add_full_model_gradient_clipping(torch.optim.AdamW)(\n",
        "                params, cfg.SOLVER.BASE_LR\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError(f\"no optimizer type {optimizer_type}\")\n",
        "        if not cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == \"full_model\":\n",
        "            optimizer = maybe_add_gradient_clipping(cfg, optimizer)\n",
        "        return optimizer\n",
        "\n",
        "    @classmethod\n",
        "    def ema_test(cls, cfg, model, evaluators=None):\n",
        "        # model with ema weights\n",
        "        logger = logging.getLogger(\"detectron2.trainer\")\n",
        "        if cfg.MODEL_EMA.ENABLED:\n",
        "            logger.info(\"Run evaluation with EMA.\")\n",
        "            with apply_model_ema_and_restore(model):\n",
        "                results = cls.test(cfg, model, evaluators=evaluators)\n",
        "        else:\n",
        "            results = cls.test(cfg, model, evaluators=evaluators)\n",
        "        return results\n",
        "\n",
        "    @classmethod\n",
        "    def test_with_TTA(cls, cfg, model):\n",
        "        logger = logging.getLogger(\"detectron2.trainer\")\n",
        "        logger.info(\"Running inference with test-time augmentation ...\")\n",
        "        model = DiffusionInstWithTTA(cfg, model)\n",
        "        evaluators = [\n",
        "            cls.build_evaluator(\n",
        "                cfg, name, output_folder=os.path.join(cfg.OUTPUT_DIR, \"inference_TTA\")\n",
        "            )\n",
        "            for name in cfg.DATASETS.TEST\n",
        "        ]\n",
        "        if cfg.MODEL_EMA.ENABLED:\n",
        "            cls.ema_test(cfg, model, evaluators)\n",
        "        else:\n",
        "            res = cls.test(cfg, model, evaluators)\n",
        "        res = OrderedDict({k + \"_TTA\": v for k, v in res.items()})\n",
        "        return res\n",
        "\n",
        "    def build_hooks(self):\n",
        "        \"\"\"\n",
        "        Build a list of default hooks, including timing, evaluation,\n",
        "        checkpointing, lr scheduling, precise BN, writing events.\n",
        "\n",
        "        Returns:\n",
        "            list[HookBase]:\n",
        "        \"\"\"\n",
        "        cfg = self.cfg.clone()\n",
        "        cfg.defrost()\n",
        "        cfg.DATALOADER.NUM_WORKERS = 0  # save some memory and time for PreciseBN\n",
        "\n",
        "        ret = [\n",
        "            hooks.IterationTimer(),\n",
        "            EMAHook(self.cfg, self.model) if cfg.MODEL_EMA.ENABLED else None,  # EMA hook\n",
        "            hooks.LRScheduler(),\n",
        "            hooks.PreciseBN(\n",
        "                # Run at the same freq as (but before) evaluation.\n",
        "                cfg.TEST.EVAL_PERIOD,\n",
        "                self.model,\n",
        "                # Build a new data loader to not affect training\n",
        "                self.build_train_loader(cfg),\n",
        "                cfg.TEST.PRECISE_BN.NUM_ITER,\n",
        "            )\n",
        "            if cfg.TEST.PRECISE_BN.ENABLED and get_bn_modules(self.model)\n",
        "            else None,\n",
        "        ]\n",
        "\n",
        "        # Do PreciseBN before checkpointer, because it updates the model and need to\n",
        "        # be saved by checkpointer.\n",
        "        # This is not always the best: if checkpointing has a different frequency,\n",
        "        # some checkpoints may have more precise statistics than others.\n",
        "        if comm.is_main_process():\n",
        "            ret.append(hooks.PeriodicCheckpointer(self.checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD))\n",
        "\n",
        "        def test_and_save_results():\n",
        "            self._last_eval_results = self.test(self.cfg, self.model)\n",
        "            return self._last_eval_results\n",
        "\n",
        "        # Do evaluation after checkpointer, because then if it fails,\n",
        "        # we can use the saved checkpoint to debug.\n",
        "        ret.append(hooks.EvalHook(cfg.TEST.EVAL_PERIOD, test_and_save_results))\n",
        "\n",
        "        if comm.is_main_process():\n",
        "            # Here the default print/log frequency of each writer is used.\n",
        "            # run writers in the end, so that evaluation metrics are written\n",
        "            ret.append(hooks.PeriodicWriter(self.build_writers(), period=20))\n",
        "        return ret\n",
        "\n",
        "\n",
        "def setup(args):\n",
        "    \"\"\"\n",
        "    Create configs and perform basic setups.\n",
        "    \"\"\"\n",
        "    cfg = get_cfg()\n",
        "    add_diffusioninst_config(cfg)\n",
        "    add_model_ema_configs(cfg)\n",
        "    cfg.merge_from_file(args.config_file)\n",
        "    cfg.merge_from_list(args.opts)\n",
        "    cfg.freeze()\n",
        "    default_setup(cfg, args)\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    cfg = setup(args)\n",
        "\n",
        "    if args.eval_only:\n",
        "        model = Trainer.build_model(cfg)\n",
        "        kwargs = may_get_ema_checkpointer(cfg, model)\n",
        "        if cfg.MODEL_EMA.ENABLED:\n",
        "            EMADetectionCheckpointer(model, save_dir=cfg.OUTPUT_DIR, **kwargs).resume_or_load(cfg.MODEL.WEIGHTS,\n",
        "                                                                                              resume=args.resume)\n",
        "        else:\n",
        "            DetectionCheckpointer(model, save_dir=cfg.OUTPUT_DIR, **kwargs).resume_or_load(cfg.MODEL.WEIGHTS,\n",
        "                                                                                           resume=args.resume)\n",
        "        res = Trainer.ema_test(cfg, model)\n",
        "        if cfg.TEST.AUG.ENABLED:\n",
        "            res.update(Trainer.test_with_TTA(cfg, model))\n",
        "        if comm.is_main_process():\n",
        "            verify_results(cfg, res)\n",
        "        return res\n",
        "\n",
        "    trainer = Trainer(cfg)\n",
        "    trainer.resume_or_load(resume=args.resume)\n",
        "    return trainer.train()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = default_argument_parser().parse_args()\n",
        "    print(\"Command Line Args:\", args)\n",
        "    launch(\n",
        "        main,\n",
        "        args.num_gpus,\n",
        "        num_machines=args.num_machines,\n",
        "        machine_rank=args.machine_rank,\n",
        "        dist_url=args.dist_url,\n",
        "        args=(args,),\n",
        "    )\n"
      ]
    }
  ]
}